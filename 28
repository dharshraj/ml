S-ALGORIYHM


data = [
    ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes'],
    ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']
]

num_attributes = len(data[0]) - 1

hypothesis = ['ϕ'] * num_attributes

for instance in data:
    if instance[-1] == 'Yes':  
        for i in range(num_attributes):
            if hypothesis[i] == 'ϕ':
                hypothesis[i] = instance[i]
            elif hypothesis[i] != instance[i]:
                hypothesis[i] = '?'

print("The most specific hypothesis is:", hypothesis)



1)linear regression

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

np.random.seed(42)
X = 2 * np.random.rand(100, 1) 
y = 4 + 3 * X + np.random.randn(100, 1)  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Intercept:", model.intercept_[0])
print("Slope:", model.coef_[0][0])
print("Mean Squared Error:", mse)
print("R² Score:", r2)

plt.scatter(X_test, y_test, color='blue', label='Actual data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')
plt.title('Linear Regression: Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()


2)logistic regression

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = (4 + 3 * X + np.random.randn(100, 1) > 8).astype(int).ravel() 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

plt.scatter(X_test, y_test, color='blue', label='Actual class')
plt.scatter(X_test, y_prob, color='red', label='Predicted probability')
plt.title('Logistic Regression: Probability vs Input')
plt.xlabel('X')
plt.ylabel('Probability of Class 1')
plt.legend()
plt.grid(True)
plt.show()



3)Polynomial Regression

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 5 + 2 * X + X**2 + np.random.randn(100, 1)  
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R² Score:", r2)

X_plot = np.linspace(0, 2, 100).reshape(100, 1)
X_plot_poly = poly.transform(X_plot)
y_plot = model.predict(X_plot_poly)

plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Polynomial regression curve')
plt.title('Polynomial Regression (Degree 2)')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

4)KNN

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

k = 3
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

5)Naive Bayes

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

6)EM (Expectation Maximization)

import numpy as np
from scipy.stats import norm
data = np.hstack([np.random.normal(0, 1, 100), np.random.normal(5, 1, 100)])
k = 2
means = np.array([0, 5], dtype=float)
variances = np.array([1, 1], dtype=float)
weights = np.array([0.5, 0.5])
for _ in range(10):  
    resp = np.array([weights[i] * norm.pdf(data, means[i], np.sqrt(variances[i])) for i in range(k)])
    resp /= resp.sum(axis=0)
    for i in range(k):
        weights[i] = resp[i].mean()
        means[i] = (resp[i] @ data) / resp[i].sum()
        variances[i] = (resp[i] @ (data - means[i])**2) / resp[i].sum()
print("Means:", means)
print("Variances:", variances)
print("Weights:", weights)

7)candidate elimination

import csv

def more_general(h1, h2):
    return all(x == '?' or x == y for x, y in zip(h1, h2))

def consistent(hypothesis, example):
    return all(h == '?' or h == e for h, e in zip(hypothesis, example))

def candidate_elimination(examples):
    attributes = len(examples[0]) - 1
    S = ['Ø'] * attributes
    G = [['?'] * attributes]

    for example in examples:
        inputs, output = example[:-1], example[-1]

        if output == 'Yes':
            G = [g for g in G if consistent(g, inputs)]

            for i in range(attributes):
                if S[i] == 'Ø':
                    S[i] = inputs[i]
                elif S[i] != inputs[i]:
                    S[i] = '?'
        else: 
            new_G = []
            for g in G:
                if consistent(g, inputs):
                    for i in range(attributes):
                        if g[i] == '?':
                            for value in set(e[i] for e in examples if e[-1] == 'Yes'):
                                if value != inputs[i]:
                                    new_h = g.copy()
                                    new_h[i] = value
                                    if more_general(new_h, S):
                                        new_G.append(new_h)
                else:
                    new_G.append(g)
            G = new_G

    final_G = []
    for g in G:
        if not any(more_general(other, g) and other != g for other in G):
            final_G.append(g)

    return S, final_G

examples = [
    ['Big', 'Red', 'Circle', 'No'],
    ['Small', 'Red', 'Triangle', 'No'],
    ['Big', 'Red', 'Circle', 'Yes'],
    ['Small', 'Red', 'Circle', 'No'],
    ['Small', 'Blue', 'Circle', 'Yes'],
]

S, G = candidate_elimination(examples)

print("Final Specific Hypothesis (S):", S)
print("Final General Hypotheses (G):")
for g in G:
    print(g)


8)Perseptron Algorithm

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data[:, :2] 
y = iris.target

y = np.where(y == 0, 1, -1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

perceptron = Perceptron(max_iter=1000, eta0=0.01, random_state=42)
perceptron.fit(X_train, y_train)

y_pred = perceptron.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")













